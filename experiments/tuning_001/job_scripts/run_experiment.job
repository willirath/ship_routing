#!/usr/bin/env bash
# shellcheck disable=SC2034,SC2155,SC2016
#SBATCH --job-name=tune
#SBATCH --ntasks=101
#SBATCH --cpus-per-task=8
#SBATCH --mem-per-cpu=5G
#SBATCH --time=04:00:00
#SBATCH --qos=express  # larger cpu count possible
#SBATCH --partition=base
#SBATCH --output=slurm_%j.out
#SBATCH --error=slurm_%j.err
#
# Usage: sbatch --export=LON_START=-75.0,LON_END=-12.0,LAT_START=25.0,JOURNEY_NAME="Atlantic" run_experiment.job

# Script: Random hyperparameter testing
# 1. Start persistent Redis server for result aggregation
# 2. Generate N random experiment configurations per route (forward + backward)
#    Tune parameter ranges in the configuration sections below (before "Load compiler environment").
# 3. Batch commands into groups and execute in parallel across available tasks

# Configuration
NRUNS=1000
RESULTS_FILE="results/results_${SLURM_JOB_ID}.msgpack"

# Parallelization
EXECUTOR_TYPE="process"
NUM_WORKERS=8
MAX_RUNTIME_PER_COMMAND="1000"  # seconds

currents_path="data_large/cmems_mod_glo_phy_my_0.083deg_P1D-m_time_2021_lat_+10_+65_lon_-100_+010_uo-vo.zarr"
waves_path="data_large/cmems_mod_glo_wav_my_0.2deg_PT3H-i_time_2021_lat_+10_+65_lon_-100_+010_VHM0-VMDR.zarr"
winds_path="data_large/cmems_obs-wind_glo_phy_my_l4_0.125deg_PT1H_time_2021_lat_+10_+65_lon_-100_+010_eastward_wind-northward_wind.zarr"

# Route parameters
LON_START=${LON_START:--80.5}
LON_END=${LON_END:--11.0}
LAT_START=${LAT_START:-30.0}
LAT_END=${LAT_END:-50.0}
TIME_START=${TIME_START:-"2021-01-01T00:00"}
SPEED_KNOTS=${SPEED_KNOTS:-10.0}
JOURNEY_NAME=${JOURNEY_NAME:-"Journey"}
JOURNEY_NAME_FORWARD="${JOURNEY_NAME}_forward"
JOURNEY_NAME_BACKWARD="${JOURNEY_NAME}_backward"

# All parameters as arrays (single-element = non-random, multi-element = random)
# Constants
LOG_DIRS=("runs")
SELECTION_ACCEPTANCE_RATES_WARMUP=(0.3)
MUTATION_WIDTH_FRACTIONS_WARMUP_SINGLE=(0.9)
MUTATION_DISPLACEMENT_FRACTIONS_WARMUP_SINGLE=(0.2)
MUTATION_WIDTH_FRACTIONS_SINGLE=(0.9)
MUTATION_DISPLACEMENT_FRACTIONS_SINGLE=(0.1)
NUM_ELITES_VALUES=(2)
GD_ITERATIONS_VALUES=(2)
LEARNING_RATES_TIME=(0.5)
LEARNING_RATES_SPACE=(0.5)
TIME_INCREMENTS=(1200.0)
DISTANCE_INCREMENTS=(10000.0)
TIME_RESOLUTION_HOURS_VALUES=(6.0)
TIME_ENDS=("")

# Random sampling arrays
HAZARD_PENALTY_MULTIPLIER_VALUES=(0 100.0)
SELECTION_QUANTILES=(0.1 0.25)
SELECTION_ACCEPTANCE_RATES=(0.0 0.25)
POPULATION_SIZES=(128 256)
GENERATIONS=(1 2 4)
MUTATION_ITERATIONS=(1 3)
GD_ITERATIONS_ARRAY=(1 2)
CROSSOVER_STRATEGIES=("minimal_cost" "random")
CROSSOVER_ROUNDS=(0 1 2)
MUTATION_WIDTH_FRACTIONS_WARMUP=(0.99)
MUTATION_DISPLACEMENT_FRACTIONS_WARMUP=(0.1 0.25)

# Adaptation parameters
ENABLE_ADAPTATION=(true false)
ADAPTATION_SCALE_W_VALUES=(0.5 0.8)
ADAPTATION_SCALE_D_VALUES=(0.707 0.894)

# Load compiler environment
module load gcc12-env

# Configure HTTP proxy for package downloads
export http_proxy=http://10.0.7.235:3128
export https_proxy=http://10.0.7.235:3128
export ftp_proxy=http://10.0.7.235:3128
export HTTP_PROXY=http://10.0.7.235:3128
export HTTPS_PROXY=http://10.0.7.235:3128
export FTP_PROXY=http://10.0.7.235:3128

# Initialize Pixi environment for Python dependencies
eval "$(pixi shell-hook)"

# Start Redis server on dedicated node
# Configure Redis parameters and launch server in background
REDIS_PORT=6379
REDIS_DIR="/tmp/redis_data_${SLURM_JOB_ID}"
REDIS_PASSWORD=$(openssl rand -base64 32)
mkdir -p "$REDIS_DIR"

srun --ntasks=1 --exclusive --export=ALL -c "${SLURM_CPUS_PER_TASK}" --job-name=redis_server bash -c "
    echo \$(hostname) > .redis_host_${SLURM_JOB_ID}
    redis-server --port ${REDIS_PORT} \
                 --protected-mode no \
                 --bind 0.0.0.0 \
                 --requirepass '${REDIS_PASSWORD}' \
                 --save 60 1000 \
                 --appendonly yes \
                 --appendfsync everysec \
                 --dir ${REDIS_DIR}
" &

# Get Redis host from the file written by the Redis task
sleep 5
REDIS_HOST=$(cat .redis_host_"${SLURM_JOB_ID}")
echo "Redis running on ${REDIS_HOST}:${REDIS_PORT} (password protected)"

# Function to batch commands for parallel execution
run_batched() {
	mapfile -t cmds
	local batch_size=$(( (${#cmds[@]} + SLURM_NTASKS - 2) / (SLURM_NTASKS - 1) ))
	printf '%s\n' "${cmds[@]}" | \
		xargs -d '\n' -L${batch_size} -P$((SLURM_NTASKS-1)) srun --ntasks=1 \
			--exclusive --export=ALL -c "${SLURM_CPUS_PER_TASK}" bash -c \
			'for cmd in "$@"; do bash -c "$cmd"; done' bash
}

# Helper function to get a random unsigned integer
random_int() {
	od -An -tu4 -N4 /dev/urandom | tr -d ' '
}

# Helper function to sample from array (random if >1 element, direct if =1)
sample_from_array() {
	local -n arr=$1  # nameref to array
	if [ ${#arr[@]} -eq 1 ]; then
		echo "${arr[0]}"
	else
		echo "${arr[$(random_int) % ${#arr[@]}]}"
	fi
}

# Function to generate random experiment commands for a route
generate_commands() {
	local lon_start=$1 lon_end=$2 lat_start=$3 lat_end=$4
	local time_start=$5 speed_knots=$6 num_samples=$7 journey_name=$8

	# Generate N random samples
	for ((i=0; i<num_samples; i++)); do
		# Sample from each parameter array (length determines if random or fixed)
		local log_dir=$(sample_from_array LOG_DIRS)
		local hazard_penalty_multiplier=$(sample_from_array HAZARD_PENALTY_MULTIPLIER_VALUES)
		local sel_quant=$(sample_from_array SELECTION_QUANTILES)
		local sel_accept=$(sample_from_array SELECTION_ACCEPTANCE_RATES)
		local sel_accept_warmup=$(sample_from_array SELECTION_ACCEPTANCE_RATES_WARMUP)
		local pop_size=$(sample_from_array POPULATION_SIZES)
		local rand_seed=$(random_int)
		local gen=$(sample_from_array GENERATIONS)
		local mut_iter=$(sample_from_array MUTATION_ITERATIONS)
		local gd_iter=$(sample_from_array GD_ITERATIONS_ARRAY)
		local gd_iterations=$(sample_from_array GD_ITERATIONS_VALUES)
		local cross_strat=$(sample_from_array CROSSOVER_STRATEGIES)
		local cross_rounds=$(sample_from_array CROSSOVER_ROUNDS)
		local mut_width=$(sample_from_array MUTATION_WIDTH_FRACTIONS_SINGLE)
		local mut_disp=$(sample_from_array MUTATION_DISPLACEMENT_FRACTIONS_SINGLE)
		local mut_width_warmup=$(sample_from_array MUTATION_WIDTH_FRACTIONS_WARMUP)
		local mut_disp_warmup=$(sample_from_array MUTATION_DISPLACEMENT_FRACTIONS_WARMUP)
		local num_elites=$(sample_from_array NUM_ELITES_VALUES)
		local learning_rate_time=$(sample_from_array LEARNING_RATES_TIME)
		local learning_rate_space=$(sample_from_array LEARNING_RATES_SPACE)
		local time_increment=$(sample_from_array TIME_INCREMENTS)
		local distance_increment=$(sample_from_array DISTANCE_INCREMENTS)
		local time_resolution_hours=$(sample_from_array TIME_RESOLUTION_HOURS_VALUES)
		local time_end=$(sample_from_array TIME_ENDS)
		local enable_adaptation=$(sample_from_array ENABLE_ADAPTATION)
		local adaptation_scale_W=$(sample_from_array ADAPTATION_SCALE_W_VALUES)
		local adaptation_scale_D=$(sample_from_array ADAPTATION_SCALE_D_VALUES)

		echo "timeout ${MAX_RUNTIME_PER_COMMAND} ship-routing" \
			"--currents-path ${currents_path}" \
			"--waves-path ${waves_path}" \
			"--winds-path ${winds_path}" \
			"--engine 'zarr'" \
			"--random-seed ${rand_seed}" \
			"--population-size ${pop_size}" \
			"--selection-acceptance-rate-warmup ${sel_accept_warmup}" \
			"--mutation-width-fraction-warmup ${mut_width_warmup}" \
			"--mutation-displacement-fraction-warmup ${mut_disp_warmup}" \
			"--generations ${gen}" \
			"--offspring-size ${pop_size}" \
			"--selection-quantile ${sel_quant}" \
			"--selection-acceptance-rate ${sel_accept}" \
			"--mutation-width-fraction ${mut_width}" \
			"--mutation-displacement-fraction ${mut_disp}" \
			"--mutation-iterations ${mut_iter}" \
			"--crossover-strategy ${cross_strat}" \
			"--crossover-rounds ${cross_rounds}" \
			"--hazard-penalty-multiplier ${hazard_penalty_multiplier}" \
			"--num-elites ${num_elites}" \
			"--gd-iterations ${gd_iterations}" \
			"--learning-rate-time ${learning_rate_time}" \
			"--learning-rate-space ${learning_rate_space}" \
			"--time-increment ${time_increment}" \
			"--distance-increment ${distance_increment}" \
			"--executor-type ${EXECUTOR_TYPE}" \
			"--num-workers ${NUM_WORKERS}" \
			"--log-dir ${log_dir}" \
			"--redis-host ${REDIS_HOST}" \
			"--redis-port ${REDIS_PORT}" \
			"--redis-password '${REDIS_PASSWORD}'" \
			"--journey-name ${journey_name}" \
			"--lon-wp ${lon_start} --lon-wp ${lon_end}" \
			"--lat-wp ${lat_start} --lat-wp ${lat_end}" \
			"--time-start ${time_start}" \
			"--speed-knots ${speed_knots}" \
			"--time-resolution-hours ${time_resolution_hours}" \
			"--enable-adaptation=${enable_adaptation}" \
			"--adaptation-scale-W ${adaptation_scale_W}" \
			"--adaptation-scale-D ${adaptation_scale_D}"
	done
}

# forward route
generate_commands "${LON_START}" "${LON_END}" "${LAT_START}" "${LAT_END}" \
	"${TIME_START}" "${SPEED_KNOTS}" "${NRUNS}" "${JOURNEY_NAME_FORWARD}" | run_batched

# backward route (reversed waypoints)
generate_commands "${LON_END}" "${LON_START}" "${LAT_END}" "${LAT_START}" \
	"${TIME_START}" "${SPEED_KNOTS}" "${NRUNS}" "${JOURNEY_NAME_BACKWARD}" | run_batched

# Extract results from Redis
# Wait for jobs to finish and extract results
sleep 30
echo "All jobs completed. Extracting results from Redis..."
./scripts/extract_redis_results.py \
	--redis-host "${REDIS_HOST}" \
	--redis-port "${REDIS_PORT}" \
	--redis-password "${REDIS_PASSWORD}" \
	--output "${RESULTS_FILE}"

# Shutdown Redis gracefully
redis-cli -h "${REDIS_HOST}" -p "${REDIS_PORT}" \
	-a "${REDIS_PASSWORD}" SHUTDOWN SAVE

# Print job resource summary
jobinfo
